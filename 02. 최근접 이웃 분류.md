### 02. 최근접 이웃 분류

>  *2019-1학기 머신러닝 수업을 통해 배운 것을 정리
>
> 교재: R을 활용한 머신러닝 / 저자: Brett Lantz 

<br>

#### 1. 최근접 이웃 분류의 이해 

- 최근접 이웃 분류기는 레이블이 없는 예시를 레이블된 유사한 예시의 클래스로 할당해 분류

(레이블이 없는 예시 = 테스트 데이터, 레이블된 유사한 예시의 클래스 = 훈련 데이터)

- k-nn의 이름은, 예시의 k-최근접 이웃 정보를 이용해 레이블이 없는 예시를 분류한다는 사실에서 붙여졌다

<br>

##### 예시

- 정지 영상 및 동영상에서 광학 글자 인식과 얼굴 인식을 포함하는 컴퓨터 비전 응용

- 영화나 음악 추천에 대한 개인별 선호 예측

- 특정 단백질 및 질병 발견에 사용 가능한 유전자 데이터의 패턴 인식

<br>

- 추가 설명
  - 일반적으로 특징과 목표 클래스 간에 관계가 많고 복잡하거나 이해하기가 매우 어렵지만, **유사한 클래스 유형의 아이템들이 상당히 동질적인 경향을 띤다**면 분류 작업에 최근접 이웃 분류기가 제격이다.
  - 하지만 데이터에 잡음이 많고, 그룹 간에 명혹한 구분이 없다면 최근접 이웃 알고리즘은 클래스 경계를 식별하기 위해 어려움을 겪음

<br>

#### 2. K-NN 알고리즘 K-Nearest Neighbors

> 분류를 위한 최근접 이웃의 대표적인 방법

<br>

##### 2.1 의미

```shell
Given a collection of data points and a query point in an m-dimensional metric space, find the data point that is closet to the query point -> Supervised Machine learning ; Classification
```

- 훈련을 하지 않고, 데이터를 가지고 있다가 해당 테스트 데이터가 입력되면, 입력된 테스트 데이터를 훈련 데이터를 바탕으로 분류함

- 학습 데이터를 가지고만 있다가 새로운 데이터가 투입되면 해당 데이터에 대한 분류가 이루어진다

<br>

**2.2 장점**

- 단순하고 효율적임(학습비용이 0이기 때문)

- 기저 데이터 분포에 대한 가정을 하지 않는다 = 비모수이고, 분포에 대한 가정이 없다(parameter 없음, 학습을 안해서)

- 훈련단계가 빠르다

- 어떤 데이터가 주어져도 해당 사례에 대한 유사성을 측정할 수 있다

<br>

**2.3 단점**

- 대용량 데이터에 대한 계산 효율성이 매우 떨어진다

- 성능이 차원에 달려 있다

- 모델을 생성하지 않아 특징과 클래스 간의 관계를 이해하는 능력이 제약된다 --> 모델이 없으므로 분류된 결과에 대한 해석이 제약된다

- 적절한 k의 선택이 필요하다

- 분류 단계가 느리다(훈련을 하지 않기 때문에)

<br>

cf. `k-means`의 `k`와 차이가 있다

k-means의 k는 군집의 개수를 의미하며, k-nn은 테스트 데이터를 분류할 때, 참고(reference)할 훈련 데이터의 개수를 의미한다.

<br>

- `k`의 의미
  - `k`는 `최근접 이웃의 개수`를 임의로 사용해도 된다는 것을 의미하는 변수 항목이며, k가 선택된 이후 알고리즘은 여러 범주로 분류돼 명목 변수로 레이블된 예시들로 구성된 훈련 데이터셋을 필요로 한다

<br>

**2.4 예시**

- 재료별로 두가지 특징만 평가하고,  각 재료에 과일, 채소, 단백질이라는 세 가지 음식 종류 중 하나로 레이블을 붙인다. 이후 토마토(테스트 데이터)는 과일인가? 채소인가?의 질문을 풀기 위해 k-nn 방법 이용

[![img](https://postfiles.pstatic.net/MjAxOTA2MjdfMTMw/MDAxNTYxNTk0MTg3MTM1.kQBniBa0J2ISyQOOztBvz3wSrC7dVWyzspVdlR0miV4g.Z-LQO6hW-Nv7ztxMebol0fdkxdQiIjGYgiPL0SWngE0g.PNG.bosongmoon/image.png?type=w773)](https://blog.naver.com/PostView.nhn?blogId=bosongmoon&logNo=221572875964&parentCategoryNo=&categoryNo=69&viewDate=&isShowPopularPosts=false&from=postList#)

- **거리로 유사도 측정**
  - 토마토의 최근접 이웃을 찾으려면 거리 함수를 사용해야 한다.
  - 거리 함수: 유클리드 거리 `Euclidean distance` - 최단 직선로를 의미하는 일직선 거리(일반적으로 두 사이의 차이를 제곱해서 루트)

 

![img](https://postfiles.pstatic.net/MjAxOTA2MjdfMjQz/MDAxNTYxNTk3MjEwNjQz.M0HVwevt-mwD85TDowsMwx9yJMJ8eZbp5CA-ANsWhzYg.-N5QsYCePDSwkllAa1roQrnOgZB8JyoXcHsfgK0h08Eg.PNG.bosongmoon/image.png?type=w773)

[![img](https://postfiles.pstatic.net/MjAxOTA2MjdfMzgg/MDAxNTYxNTk5NTgyMDAx.fr3cp4mLUyIDVWiVTqNTvg3R5f6P68E3r5tZITE49pMg.yj5Ewz9NKdOIQny2uTENMM7K-SGmk6vsTsIcfaQFVcUg.PNG.bosongmoon/image.png?type=w773)](https://blog.naver.com/PostView.nhn?blogId=bosongmoon&logNo=221572875964&parentCategoryNo=&categoryNo=69&viewDate=&isShowPopularPosts=false&from=postList#)

- 만약 k =1 이라면, 1개의 최근접 이웃을 가지고 토마토를 분류하겠다라는 의미

- k = 1일때는 `오렌지`가 토마토의 최근접 이웃으로 거리가 1.4이다. 1-NN일때는, 토마토는 과일로 분류함

- k = 3일 땐, `오렌지(1.4)`, `포도(2.2)`, `견과(3.6)`가 투표를 하고, 이웃 중 다수의 클래스가 과일이기 때문에, 토마토는 다시 과일로 분류됨(3표 중 2표)

<br>

**2.5 적절한 K의 선택**

- `k-nn`에서 사용할 이웃의 개수는 모델이 미래 데이터에 대해 일반화되는 능력을 결정한다.

- 훈련 데이터에 대한 과적합과 과소적합사이의 균형은 트레이드 오프 문제임

- k를 큰값으로 선택시, 작더라도 중요한 패턴을 무시할 가능성 있음(잡음이 많은 데이터로 인한 영향은 감소함)

- k를 작은값으로 선택시, 잡음이 있는 데이터나 이상치가 예시의 분류에 과도한 영향을 미침

- 즉, **k값이 작을 수록 더 복잡한 결정 경계가 만들어져 훈련 데이터에 세밀하게 맞춰진다**(과적합 - 잡음과 이상치에 분류가 휘둘릴 가능성이 높음)

- 입력치의 dimension이 많아지면, 차원의 저주에 빠진다 ; 차원이 데이터 수 보다 많을 경우, 제대로 훈련이 불가능

<br>

*****

**K-NN 사용을 위한 데이터 준비: 데이터 표준화**

- k-nn 알고리즘에 적용하기 전에, 데이터는 **표준 범위로 변환**되어야 한다. 그 이유는 유클리디 거리를 이용하기 때문. 특정한 특징의 값이 다른 특징에 비해 크다면 거리 측정치는 큰 범위를 갖는 특징에 따라 크게 좌우된다(범위를 1-10 사이로 하면 상관없는데, 만약 하나는 1-10, 다른 하나가 100-1000이면 거리 측정에 문제가 생김)

<br>

**데이터 표준화하는 방법(2가지)**

[![img](https://postfiles.pstatic.net/MjAxOTA2MjdfMjg0/MDAxNTYxNjEwMTE2MzIz.xaSxT0MichhUNxy1cKQJ2FS8rMydvKwGqp4rE1mnP1Ug.eC1zf65keDDqrgfIqfziivkdFXR8TrA3wMPnYgFly2cg.PNG.bosongmoon/image.png?type=w773)](https://blog.naver.com/PostView.nhn?blogId=bosongmoon&logNo=221572875964&parentCategoryNo=&categoryNo=69&viewDate=&isShowPopularPosts=false&from=postList#)

- min-max normalization: 최소-최대 정규화
  - 모든 값이 0에서 1사이로 변화함

- z-score standardization z-점수 표준화
  - 표준 정규 분포로 표준화하는 것을 의미

---

<br>

**2.6 K-NN 알고리즘이 게으른 이유**

- 흔히 lazy learning 게으른 학습이라고 부른다. 그 이유는 추상화가 일어나지 않기 때문이다

- **훈련 데이터를 글자 그대로 저장하기만 하기 때문에**, 훈련 단계가 매우 빠르게 일어나게 되는데, 실제 훈련단계에서는 아무것도 훈련하지 않는다 / 학습 데이터를 가지고만 있다가 새로운 데이터가 투입되면 해당 데이터에 대한 분류가 이루어진다

- 추상화된 모델보다 훈련 인스턴스에 많이 의존하기 때문에, 게으른 학습은 인스턴스 기반 학습 또는 암기학습이라고도 한다

- 인스턴스 기반 학습자는 모델을 만들지 않기 때문에, 비모수(non-parametric)의 부류라고 한다 = 데이터에 대해 학습하는 파라미터가 없다

<br>

#### 3. K-NN 특징 총 정리

- 학습 데이터를 가지고만 있다가 새로운 데이터가 투입되면 해당 데이터에 대한 분류가 이루어진다

- 학습 데이터로 모형을 만드는 과정이 없다 = parameter 도 없다

- 통상적인 지도학습 모형은 학습 데이터가 제시되면 계산을 한 결과를 가지고 있는데, k-NN은 아니다. 테스트 데이터가 투입되어야 계산이 시작된다

- 훈련 데이터가 그 자체로 지식(knowledge)을 의미한다. (memory-based learning, instance-based learning)