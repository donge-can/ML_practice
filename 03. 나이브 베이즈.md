

### 03. 나이브 베이즈

>  *2019-1학기 머신러닝 수업을 통해 배운 것을 정리
>
>  교재: R을 활용한 머신러닝 / 저자: Brett Lantz 



#### 1.나이브 베이즈 이해

- 베이지안 기법 기반의 분류기는 훈련 데이터를 활용해 특징 값이 제공하는 증거를 기반으로 **결과가 관측될 확률을 계산**한다. 나중에 분류기가 레이블이 없는 데이터에 적용될 때 결과가 관측될 확률을 이용해서 새로운 특징에 가장 유력한 클래스를 예측한다.

- 구체적인 예로, 날씨의 예측은 확률적 방법을 기반으로 한다. 미래의 사건(강수 가능)을 추정하기 위해 과거의 사건 데이터를 사용한다. 날씨의 경우,  비가 올 확률은 강수가 발생했던 대기 조건과 유사한 대기 조건에 대한 이전의 날들의 비율을 의미한다.



#### 예시

- 스팸 이메일 필터링과 같은 텍스트 분류

- 컴퓨터 네트워크에서 침입이나 비정상행위 탐지

- 일련의 관찰된 증상에 대한 의학적 질병 진단





### 2. 베이지안 기법의 기본 개념

> 조건부 확률

[![img](https://postfiles.pstatic.net/MjAxOTA2MjhfMjM1/MDAxNTYxNzAwODExNjMy.5-JwO8wQEtu9N47968t10Q5epPiRhj19jK9YYN3Gc-Yg.S3OoVHTzqrqHhJ7tXkA0CJyzhOMqiId7n39TaEJILJog.PNG.bosongmoon/image.png?type=w773)](https://blog.naver.com/PostView.nhn?blogId=bosongmoon&logNo=221578151847&parentCategoryNo=&categoryNo=69&viewDate=&isShowPopularPosts=false&from=postList#)

- 비아그라를 포함하고 있는 단어가 스팸일 확률

- P(스팸|비아그라) =( P(비아그라|스팸) * P(스팸))/P(비아그라) Posterior probability

- P(스팸|비아그라) : 사후 확률

- P(비아그라|스팸) : 우도 Likelihood

- P(스팸) : 사전확률 Prior Probability 

- P(비아그라) : 주변 우도 Marginal likelihood

<br>

- 사후 확률은 우도 확률과 사전 확률의 곱에 비례한다

\*두 사건이 완전히 관련이 없다면 `독립 사건`이라고 부른다. 독립 사건이 동시에 발생할 수 없는 것이 아니라, 단순히 한 사건의 결과를 아는 것으로 다른 사건의 결과에 대한 어떤 정보도 제공하지 못한다는 것을 의미한다. 

<br>

#### 3. 나이브 베이즈 알고리즘

- 분류 문제에 베이즈 이론을 적용한 방법

- **텍스트 분류**에 많이 이용됨

<br>

##### 3.1 장점

- 간단하고 빠르고 매우 효율적

- 잡음과 누락 데이터를 잘 처리한다

- 훈련에는 상대적으로 적은 예시가 필요하지만, 대용량의 예시에도 매우 잘 작동된다

- 예측을 위한 추정 확률을 쉽게 얻을 수 있다

<br>

##### 3.2 단점

- 모든 특징이 동등하게 중요하고 독립이라는 가정을 했지만, 이 가정이 잘못된 경우가 자주 있다.

- 위의 예) 이메일 메시지를 감시해서 스팸을 식별하려고 할 때, A라는 특징이 B 특징보다 더 중요할 때가 있으나, 이를 무시하고 계산함

- 수치 특징이 많은 데이터셋에는 이상적이지 않다

- 추정된 확률이 예측된 클래스보다 덜 신뢰할 만하다

<br>

##### 3.3 나이브 베이즈 알고리즘의 유래

> Navie 순진한 알고리즘

- 나이브 베이즈 알고리즘은 데이터에 대해 `순진한 naive` 가정을 하고 있기 때문에, 붙여진 이름이다.

- 데이터셋의 모든 특징이 동등하게 중요하고 독립적이라고 `가정`한다. 이런 가정은 대부분의 실제 응용에는 거의 맞지 않다.

- 예를 들어, 이메일 메시지를 감시해서 스팸을 식별하려고 한다면, 어떤 특징이 다른 특징보다 더 중요하다는 것은 거의 분명한 사실이다.  또한, 본문의 단어는 독립이 아니다. 단어 비아그라가 있는 메시지는 처방이나 약이란 단어도 포함할 가능성이 있다. 

<br>

##### 3.4 라플라스 추정량 Laplace estimator

- 나이브 베이즈 공식에서 확률은 조건부 확률로 이루어졌기 때문에, 우도가 0%라면 해당 사후확률을 0으로 만들어, 다른 증거를 실질적으로 무효화하고 기각하게 만든다. 이에 대한 해결책으로 '라플라스 추정량'을 사용하게 된다. * A 예시가 다른 예시들이 갖고 있는 특징들을 갖고 있지 않으면, A 예시에 대한 확률은 0이 된다. 

- 라플라스 추정량은 빈도표의 각 합계에 작은 숫자를 더하는데, 특징이 각 클래스에 대해 발생할 확률이 0이 되지 않게 보장한다. 보통 라플라스 추정량은 `1`로 설정해서 데이터에 `각 클래스 특징 조합이 최소 한번은 나타나게 보장`한다. 


<br>

##### 3.5 나이브 베이즈에서 수치 이용

- 나이브 베이즈는 데이터를 학습하기 위해 빈도표를 사용하기 때문에 행렬을 구성하는 각 클래스와 특징 값의 조합을 생성하려면 각 특징이 범주형이어야 한다. 따라서 수치 특징을 `이산화` 해 나이브 베이즈 알고리즘을 적용할 수 있다.
- `binning(비닝)` : 빈 이라는 범주에 숫자를 넣는 것
- `binning`을 사용하기 위해서는 훈련 데이터가 대용량인 경우에 이상적임

<br>

#### 4. 요약

- 나이브 베이즈 알고리즘은 **확률 테이블**을 구성해 새로운 예시가 다양한 클래스에 속할 우도를 추정하는 데 사용되게 했다. 이 확률은 종속 사건이 어떻게 연관되는지를 명시한 **베이즈 정리로 알려진 공식**을 이용해 계산된다. 