### 04. 의사결정 트리

>  *2019-1학기 머신러닝 수업을 통해 배운 것을 정리
>
>  교재: R을 활용한 머신러닝 / 저자: Brett Lantz 



#### 1. 의사결정 트리의 이해

- 의사결정 트리 학습자는 강력한 분류기로 특징과 잠재적 결과 간의 관계를 모델링 하기 위해 트리 구조 tree structure를 활용한다 :: 의사결정 트리 분류는 "예시"를 최종 예측 클래스 값으로 보내기 위해 결정을 분기시키는 구조를 사용한다. 

- 분석 과정을 나무 구조로 도형화하여 분류 분석 또는 회귀 분석을 수행



​								 [![img](https://postfiles.pstatic.net/MjAxOTA3MDVfMTA2/MDAxNTYyMjg2MTU5MjEy.5u3T7v0Rjpj0RIYy8JAMePjGz3Og8oXAxAm3chJ2VA4g.C8BFaq8ZOsggUrOOAMKOdpUW1-ig-WktFAD5pxuftPwg.PNG.bosongmoon/image.png?type=w773)](https://blog.naver.com/PostView.nhn?blogId=bosongmoon&logNo=221578260888&parentCategoryNo=&categoryNo=69&viewDate=&isShowPopularPosts=false&from=postList#)

- 위와 같은 구조를 보인다.

- `루트 노드 root node` 에서 시작해서 예시의 속성에 따라 선택이 이뤄지는 `결정 노드 decision node` 를 통과하게 된다. 이런 선택은 결정의 잠재적 결과를 나타내는 `분기 branches` 로 데이터를 분할한다. 최종 결정이 이뤄지면 트리는 `잎 노드 leaf nodes` 에서 종료된다. 

<br>



- **Tree Alghorithm**
  - 불순물을 걸러서 최대한의 방식으로 분류하는 것을 의미함



* 예시 

  - 신청자 거절 기준이 명확히 문서화되고 편향되지 않은 신용 평가 모델

  - 경영진 또는 광고 대행사와 공유될 고객 만족이나 고객 이탈과 같은 고객 행동 마케팅 연구

  - 실험실 측정, 증상, 질병, 진행률을 기반으로 하는 질병 진단

<br>

* 분할정복
  * 의사결정 트리는 재귀분할 recursive partitioning 이라고 하는 휴리스틱을 사용해서 만든다
  * 이 방법은 일반적으로 분할 정복 divide and conquer 으로도 알려져 있는데, 데이터를 부분 집합으로 나누고 그 다음 더 작은 부분집합으로 반복해서 나누며, 알고리즘이 부분 집합의 데이터가 충분히 동질적이라고 판단하거나 다른 종료 조건을 만나서 과정이 종료될 때 까지 계속되기 때문.

<br>

#### 2. C5.0 의사결정 트리 알고리즘

- 의사결정 트리의 알고리즘 중 가장 알려진 알고리즘이다.

- C4.5의 개선된 버전에 해당한다. 

<br>

* 장점
  * 대부분의 문제에 잘 실행되는 범용 분류기다.
  * 수치 특징, 명목 특징, 누락 데이터를 다룰 수 있는 고도로 자동화된 학습 과정이다
  * 중요하지 않은 특징은 제외한다.

  - 작은 데이터셋과 큰 데이터셋에 모두 사용될 수 있다
  - (상대적으로 작은 트리에 대해) 수학적 배경없이 해석될 수 있는 모델을 만든다
  - 다른 복잡한 모델보다 더 효율적이다

<br>

* 단점
  * 의사결정 트리 모델이 레벨 수가 많은 특징의 분할로 편향도리 수 있다.
  * 모델이 과적합 또는 과소적합되기 쉽다 *이게 가장 큰 문제이다. 이 문제를 해결하기 위해, 추후에 boosting, bagging, random Forest 등의 해결방안을 도입한다.

  - 축-평행 분할에 의존하기 때문에 어던 관계는 모델링이 어려울 수 있다.

  - 훈련 데이터에서 작은 변화가 결정 로직에 큰 변화를 초래할 수 있다

  - 큰 트리는 해석이 어렵고 트리가 만든 결정은 직관적이지 않아 보일 수 있다. 



#### 3. 최고의 분할 선택

> 엔트로피와 지니 불순도

**3.1 엔트로피 Entropy**

- 트리 분할 후보를 식별하기 위해서 사용할 수 있는 방법으로, **C5.0 알고리즘은 엔트로피 entropy**를 사용한다. 엔트로피는 집합 내에서 무작위성 또는 무질서를 정량화한 값이다. 엔트로피가 높은 집합은 매우 다양하며, 같은 집합에 속해있는 다른 아이템에 대해 뚜렷한 공통점이 없기 때문에 정보를 거의 제공하지 않는다. 

- 의사결정 트리는 엔트로피를 줄이는 분할을 찾고 궁극적으로 그룹 내 동질성을 증가시키려고 한다.
- 엔트로피 값이 올라갈수록 , 나에게 주는 정보가 많아진다. 따라서 엔트로피가 가장 높은 집합을 루트 노드에 설정한다.:: **정보 획득량이 높을수록 이 특징에 대해 분할한 후 동질적 그룹을 생성하기에 더 좋은 특징이다**. 

- 정보 획득량이 0인 경우 이 특징에 대해 분할을 하게 되면, 엔트로피가 감소하지 않는다. 반면 최대 정보 획득량의 경우 분할 전 엔트로피와 정보 획득량이 동일하다. 즉, 분할 후 엔트로피가 0이라는 것을 의미하며, 분할로 인해 완전히 동질적인 그룹이 만들어졌음을 의미한다. 

<br>

-  공식은 아래와 같다.

[![img](https://postfiles.pstatic.net/MjAxOTA3MDVfODMg/MDAxNTYyMjg3OTY4MDc4.PemKU_rBgFlccvpS1DPTsgYi2V2r8cIJBH7Tp0crCWQg.-PuoYoo2WSTJxWnJmqyBFBMQXmWZyf942IiZ_p7S8mIg.PNG.bosongmoon/image.png?type=w773)](https://blog.naver.com/PostView.nhn?blogId=bosongmoon&logNo=221578260888&parentCategoryNo=&categoryNo=69&viewDate=&isShowPopularPosts=false&from=postList#)



- 예시

[![img](https://postfiles.pstatic.net/MjAxOTA3MDVfNyAg/MDAxNTYyMjg3OTgzMzQy.GnPNTKvHsndqb1H56__sGm5uiqlUDhbCMfDbNZOk_7gg.oOWlmKuIXhH-cdFv9hlXX9lnFUCLslbyZbMKt9ZT3CQg.PNG.bosongmoon/image.png?type=w773)](https://blog.naver.com/PostView.nhn?blogId=bosongmoon&logNo=221578260888&parentCategoryNo=&categoryNo=69&viewDate=&isShowPopularPosts=false&from=postList#)





<br>

**3.2 지니 불순도 Gini Impurity**

- 의사결정 트리 알고리즘 중 CART 알고리즘이 사용하는 방식으로, 불순물이 적어지는 쪽으로 분류를 한다.

- 가지치기를 할수록 불순도가 내려가는 방향으로, 루트 노드를 설정한다. 

- 불순도는 집합 내 동질성이 낮을수록 높다.

<br>

- 공식은 아래와 같다

[![img](https://postfiles.pstatic.net/MjAxOTA3MDVfMjUw/MDAxNTYyMjg4MTM0MjI3.W7lT_ho-9G8yVXYkfrXvmbZ856g_yrbcT6oFOUoUVlcg.j-0CmWQJJpYFVuPIybp30J2r9ByT9XDuOWkqek-j2OEg.PNG.bosongmoon/image.png?type=w773)](https://blog.naver.com/PostView.nhn?blogId=bosongmoon&logNo=221578260888&parentCategoryNo=&categoryNo=69&viewDate=&isShowPopularPosts=false&from=postList#)

<br>

#### 4. 의사결정 트리 가지치기 Pruning

**4.1 조기 종료 early stopping / 사전 가지치기 pre-pruning**

- 사전에 특정 시점에 트리 성장 종료 (특정한 기준을 만들어 놓고 초반에 자르기) 

- 예를 들어, sample이 10개 이하로 떨어지면 가지치기를 멈추라고 할 수 있다.

- 트리가 불필요한 일을 하지 않게 해주므로 설득력 있으나, 감지하기는 힘들지만 트리가 크게 자랐다면 학습할 수 도 있는 중요한 패턴을 놓칠 수 있다.

<br>

**4.2 사후 가지치기 post-pruning**

- 의도적으로 트리를 아주 크게 자라게 한 후, 트리 크기를 좀 더 적절한 수준으로 줄이기 위해 잎 노드를 자른다. 

- 사전 가지치기 보다 효율적인 방법이다. 왜냐하면, 먼저 키우지 않고는 의사결정 트리의 최적의 깊이를 판단하기가 매우 어렵기 때문.

- CART 와 C5.0 알고리즘에서 많이 사용 