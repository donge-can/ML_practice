# 1장. 

### 0. 머신러닝

> 경험 E 을 통해서 작업 Task 를 수행하는 성능 Performance 가 향상되도록 하는 컴퓨터 프로그램
>
> A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.

### 1. 머신러닝의 세 가지 종류

> 지도 학습 Supervised learning, 비지도 학습 Unsupervised learning, 강화 학습 Reinforcement learning

<br>

#### 1.1 지도 학습 Supervised learning

> 레이블된 훈련 데이터에서 모델을 학습하여, 새로운 데이터에 대한 예측을 만드는 것

- 레이블된 데이터
- 직접 피드백
- 출력 및 미래 예측

##### 1.1.1 `분류` : 클래스 레이블 예측

> 지도 학습의 일종으로, 새로운 샘플의 범주형 클래스 레이블 예측

스팸 분류와 같은 이진 분류, 2개 이상의 클래스 레이블을 가진 경우의 분류

##### 1.1.2 `회귀` : 연속적인 출력 값 예측

> 예측 변수와 **연속적인** 반응 변수가 주어졌을 때, 출력 값을 예측하는 두 변수 사이의 관계 찾기

- `예측 변수 Predictor variable` / `설명 변수 Explanatory` / `입력 input` / `독립 변수 independent` 
- `반응 변수 Response variable` / `출력 output` / `타깃 target` 



#### 1.2 비지도 학습 Unsupervised learning

> 클러스터링, 차원축소
>
> > 알려진 출력 값이나 보상 함수의 도움을 받지 않고 의미 있는 정보를 추출하기 위해 데이터 구조를 탐색할 수 있음

- 레이블 및 타깃 없음
- 피드백 없음
- 데이터에서 숨겨진 구조 찾기

##### 1.2.1 군집 : 서브그룹 찾기

- 사전 정보 없이 쌓여 있는 그룹 정보를 의미 있는 서브 그룹 혹은 클러스터로 나누는 것

##### 1.2.2 차원 축소 : 데이터 압축

- 고차원의 데이터를 다룰 때 주로 사용

<br>

#### 1.3 강화 학습 Reinforcement learning

> 머신러닝의 또 다른 종류, 환경과 상호 작용하여 시스템의 성능을 `향상`하는 것이 목적

- 결정 과정
- 보상 시스템
- 연속된 행동에서 학습

<br>

### 2. 기본 용어와 표기법

- `열 column` : 특성 feature / 차원 dimension / 속성 attribute
- `행 row` : 관측 observation / 인스턴스 instance

#### 2.1 Model Representation

![regression](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/H6qTdZmYEeaagxL7xdFKxA_2f0f671110e8f7446bb2b5b2f75a8874_Screenshot-2016-10-23-20.14.58.png?expiry=1578355200000&hmac=7Z59gUdr1LaB25IT3d7t9x7iQuoUbhhu_jf98ik1I7U)

- 회귀 분석에서 'h' 는 가설을 의미하며, training set이 주어졌을 때 y 값을 예측하기 위한 최적의 함수 h 를 도출하는 것이 목적이다.



#### 2.2 Cost Function

>  예측값과 실제 y 값의 차이를 나타낸 함수

![cost function](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/R2YF5Lj3EeajLxLfjQiSjg_110c901f58043f995a35b31431935290_Screen-Shot-2016-12-02-at-5.23.31-PM.png?expiry=1578355200000&hmac=GcZX1qg9PLlWZ3oxs_m5g-p2BIk_t0T6YQgUpL2yT88)



- ![](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200106082019783.png)

- ![](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200106083023159.png)

  - 오차 함수의 제곱  Mean squared error / Squared error function 이라고도 불린다.

  <br>

  ##### 2.2.1 Gradient Descent

  > 경사하강법 : 비용함수의 최소값을 구하는 알고리즘

![](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200107083514584.png)

- 알파의 값이 너무 크거나 작으면 너무 많은 시간이 걸린다. 그렇다면 어떤 식으로 알파의 크기를 결정하는 것이 좋을까?
  - 하강 기울기는 지역 최소값에 가까워질수록, 더 작은 거리를 이동하게 된다. 왜냐하면, 최소값에 가까워진다는 것은 `미분계수가 0`이 되는 것이기 때문이다. 지역 최소값에 가까워질 수록 미분계수 값은 작아지게 되고, 하강 기울기는 더 조금씩 이동하게 된다. 그렇기 때문에 `α` 값을 감소할 필요가 없다.
  - ![](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200107084152058.png)

<br>

##### 2.2.2 Gradient Descent For Linear Regression

![](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200107085825070.png)

- 선형회귀에서 사용되는 비용함수의 기울기 하강은 항상 전역적 최적값을 향하게 된다. 선형 회귀의 비용함수는 항상 활 모양으로(볼록 함수로), 전역적 최적값만을 가지고 있고 지역적 최적값은 가지고 있지 않다.  
  - 전역적 최소값이 가설과 일치할 때, 자료와 잘 일치하게 된다.

![](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200107085852725.png)

- `Batch Gradient Descent` : 훈련예제에서 모든 기울기 하강의 단계를 포함한 것을 의미
  - 기울기 하강에서 미분계수를 계산할 때, 합계를 계산하게 된다. 즉, 기울기 하강의 단계를 지나다 보면, m 개의 훈련 예제들의 값을 더하게 된다. 