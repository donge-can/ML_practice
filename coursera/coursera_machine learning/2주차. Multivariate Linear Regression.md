



### 2주차. Multivariate Linear Regression



#### 1. 알아야 할 개념

- `열 벡터` / `행 벡터` :

![](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200107183643804.png)

- `다중 회귀` 

![](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200107183620197.png)



#### 2. Grdient Descent for Multiple Variables

##### 2.1 Gradient Descent for Multiple Variables

![](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200108082218249.png) ![](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200108082309013.png)



![](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200108082238726.png)



##### 2.2 feature scaling

- Feature의 단위가 다르더라도, 범위가 같다면 gradient descent는 더 빠르게 수렴할 수 있다. (적은 수의 반복으로도 수렴) 따라서 `feature scaling` 을 해주는 것이다. (`Normalization` 을 하는 이유)
- ![](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200108083329053.png)
- 위의 예시처럼`feature`의 범위가 다를 경우, cost function은 긴 타원형 모양으로 그려질 것이고, 이럴 경우 오랜 시간이 걸려서 최소값을 찾을 수 있다. 이를 방지하기 위해서, feature scale을 하는 것임
- `mean normalization` : `feature scaling`의 한 가지 방법
  - ![](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200108084127770.png)
  - (x- 평균) / 표준편차



##### 2.3 Learning Rate

- `Debugging` : How to make sure gradient descent is working correctly. How to choose learning rate 
- ![](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200108084644497.png)
  - x 축 : `gradient descent`의 동작 횟수
  - y 축 : `gradient descent`를 할 때 마다 나오는 cost function 의 값

<br>

`gradient descent` 가 수렴하는 데 필요한 반복 횟수는 경우에 따라 달라서 얼마나 반복하는지는 사전에 알기 어렵다. 수렴하는지 아닌지 자동으로 검사하는 방법 `automatic convergence test` 가 존재하기도 한다. 





#### 3. Features and Polynomial Regression (다항 회귀)

- 모델 성능을 위해서, feature을 다양한 방식으로 변형할 수 있다.
  - 여러 개의 feature을 하나의 feature로 만들 수도 있다. 예를 들어, 집의 depth와 집의 width 두 feature를 곱해 하나의 feature(집의 크기)로도 충분히 사용할 수 있다.
  - 또한, `집의 크기의 제곱`, `집의 크기의 세제곱`을 활용해 feature을 새롭게 생성해 회귀 방정식 가설을 세울 수 있다.
- 