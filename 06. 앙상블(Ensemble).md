## 06. 앙상블(Ensemble)

> 앙상블 : 여러 개의 모델을 학습시켜 그 모델들의 예측결과들을 이용해 하나의 모델보다 더 나은 값을 예측하는 방법을 의미

- 대표적인 예 : **랜덤 포레스트**, **배깅**, **부스팅**, **스태킹**
  - 랜덤 포레스트 : 여러 개의 의사결정 나무(Decisino Tree) 들을 생성한 다음, 각 개별 트리의 예측값들 중에서 가장 많은 선택을 받은 클래스로 예측하는 알고리즘



<br>



### 1. 투표 기반 분류기 (Voting Classifier)

- 투표 기반 분류기는 학습 단계에서 여러 개의 머신러닝 알고리즘 모델을 학습시킨 후, 이러한 모델들을 이용해 새로운 데이터에 대해 각 모델의 예측값을 가지고 **다수결 투표**를 통해 최종 클래스를 예측하는 방법.
- 이러한 분류기를 직접 투표 hard voting 분류기라고 함. 이러한 다수결 투표 분류기가 앙상블에 포함된 개별 분류기 중 가장 성능이 좋은 분류기 보다 정확도가 더 높을 경우가 많음.

![](https://t1.daumcdn.net/cfile/tistory/9954013B5B836E9822?download)



<br>

- 예시 : `moons` 데이터셋에 앙상블 학습 적용시키기

```python
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_moons

X, y = make_moons(n_samples=500, noise=0.30, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
```

<br>

#### Hard Voting

- `VotingClassifier` 에서 `voting='hard'` 인 경우, 각 분류기의 예측값(레이블)을 가지고 다수결 투표를 통해 최종 앙상블 예측이 이루어진다.

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

log_clf = LogisticRegression(random_state=42)
rnd_clf = RandomForestClassifier(random_state=42)
svm_clf = SVC(random_state=42)

### VotingClassifier
voting_clf = VotingClassifier(
    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],
    voting='hard')
    
voting_clf.fit(X_train, y_train)
```

```python
from sklearn.metrics import accuracy_score

for clf in (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))
```

<br>

#### Soft Voting

- `VotingClassifier` 에서 `voting='soft'` 인 경우, 각 분류기의 예측값(레이블)의 **확률**을 가지고 평균을 구한 뒤, 평균이 가장 높은 클래스로 최종 앙상블 예측이 이루어진다. 이러한 방법을 간접 투표(soft voting) 이라고 한다.

```python
log_clf = LogisticRegression(random_state=42)
rnd_clf = RandomForestClassifier(random_state=42)
svm_clf = SVC(probability=True, random_state=42)

### VotingClassifier
voting_clf = VotingClassifier(
    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],
    voting='soft')
voting_clf.fit(X_train, y_train)

```

```python
from sklearn.metrics import accuracy_score

for clf in (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))
```

<br>

<br>

### 2. 배깅과 페이스팅(Bagging and Pasting)

- `Bagging` : 학습 데이터 셋에서 랜덤하게 추출하여 모델을 각각 다르게 학습시킴(데이터 중복 허용)
  - **bootstrap aggregating** 의 줄임말이며, 중복을 하용한 리샘플링(resampling)을 부트스트래핑(bootstraping) 이라고 함
- `Pasting` : 학습 데이터 셋에서 랜덤하게 추출하여 모델을 각각 다르게 학습시키는 것은 동일하나, 데이터 중복을 허용하지 않는다.

![](https://t1.daumcdn.net/cfile/tistory/9940D7375B836ECA01?download)

- 위의 그림에서 각 모델이 학습된 후에 새로운 데이터에 대해서는 `투표기반 분류기` 와 동일하게 분류일 때는, 최빈값 즉, 가장 많은 예측 클래스로 앙상블이 예측하며, 회귀일 경우에는 각 분류기의 예측값의 평균을 계산하여 평균값을 예측값으로 한다.
- 각 모델은 전체 학습 데이터셋으로 학습시킨 것보다 편향되어 있지만, 앙상블을 통해 편향과 분산이 감소한다. 일반적으로 앙상블 학습은 전체 학습 데이터셋을 이용해 하나의 모델을 학습시킬 때와 비교해서 편향은 비슷하지만 분산은 줄어든다.



<br>

#### 2.1 사이킷런의 배깅과 페이스팅

- scikit-learn은 분류 - `BaggingClassifier` ,  회귀 - `BaggingRegressor` 제공

<br>

- 예제 - `moons` 데이터셋 로드

```python
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_moons

X, y = make_moons(n_samples=500, noise=0.30, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
```

<br>

- 단 하나의 Decision Tree 사용한 경우

```python
from sklearn.tree import DecisionTreeClassifier

tree_clf = DecisionTreeClassifier(random_state=42)
tree_clf.fit(X_train, y_train)
y_pred_tree = tree_clf.predict(X_test)
print('Accuracy =', accuracy_score(y_test, y_pred_tree))
# 결과 : Accuracy = 0.856
```

<br>

- Bagging 이용한 앙상블 학습

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

bag_clf = BaggingClassifier(
    DecisionTreeClassifier(random_state=42), n_estimators=500, # 500개의 의사결정 나무 모델
    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)
bag_clf.fit(X_train, y_train)
y_pred = bag_clf.predict(X_test)

print('Accuracy =', accuracy_score(y_test, y_pred))
# 결과 : Accuracy = 0.904
```

- `BaggingClassifier` 는 사용하는 알고리즘 모델(분류기)가 각 예측 클래스의 확률을 추정할 수 있으면, 직접 투표 대신 간접 투표(soft voting) 방식을 사용한다. 

<br>

#### 2.2 OOB(Out of Bag) 평가

- 배깅은 중복을 허용하는 리샘플링 즉, 부트스트래핑(bootstraping) 방식이기 때문에, 전체 학습 데이터셋에서 어떠한 데이터 샘플은 여러번 샘플링 되고, 또 어떠한 샘플은 전혀 샘플링 되지 않을 수가 있다. 평균적으로 학습 단계에서 전체 학습 데이터셋 중 63% 정도만 샘플링 되며([참고]([https://tensorflow.blog/%EB%9E%9C%EB%8D%A4-%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8%EC%97%90%EC%84%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0%EA%B0%80-%EB%88%84%EB%9D%BD%EB%90%A0-%ED%99%95%EB%A5%A0/](https://tensorflow.blog/랜덤-포레스트에서-데이터가-누락될-확률/))), 샘플링 되지 않은 나머지 37% 데이터 샘플들을 **OOB(Out of bag) 샘플**이라고 한다.

- 앙상블 (배깅) 모델의 학습 단계에서는 OOB 샘플이 사용되지 않기 때문에, 이러한 OOB 샘플을 검증셋(validation set) 이나 교차 검증(cross validation)에 사용할 수 있다.

- Scikit-Learn 에서는 `BaggingClassifier` 의 인자인 `oob_score =True` 로 설정하면 학습이 끝난 후, 자동으로 OOB 평가를 할 수 있다.

  - 예제

  ```python
  bag_clf = BaggingClassifier(
      DecisionTreeClassifier(random_state=42), n_estimators=500,
      bootstrap=True, n_jobs=-1, oob_score=True, random_state=40)
  bag_clf.fit(X_train, y_train)
  
  print('oob score :', bag_clf.oob_score_)
  # 결과 : oob score : 0.901
  ```

<br>

### 3. Random Patches & Random Subspace

- `BaggingClassifier` 는 특성 (feature) 샘플링 또한 `max_features` 와 `bootstrap_features` 두 개의 인자를 통해 제공한다. (랜덤하게 선택한 특성으로 학습)
- 데이터의 특성이 많은 고차원의 데이터셋을 다룰 때 적절. **학습 데이터 셋의 특성** 및 **샘플링 사용 유무**에 따라 두 종류로 나눌 수 있음
  - **Random Pathces method** : 특성 및 데이터 셋 샘플링(bootstraping) 모두 사용하는 방식
  - **Random Subspace method** : 특성만 샘플링하는 방식
    - `bootstrap=False` 이고, `bootstrap_features=True` 그리고 `max_features` 는 1.0보다 작은 값

<br>

- 이러한 특성 샘플링은 더 다양한 모델을 만들며 편향은 늘어나지만, 분산을 낮출 수 있다.

<br>

### 4. 랜덤 포레스트 (Random Forest)

- **랜덤 포레스트**(Random Forest) 는 배깅을 적용한 의사결정 나무(decision tree) 의 앙상블 / 특성 + 데이터 배깅
  - 2.1 예제에서는 `BaggingClassifier` 에 `DecisionTreeClassifier` 를 인자로 넣어 주었다. scikit-Learn 에서는 랜덤 포레스트를 간편하게 쓸 수 있도록 `RandomForestClassifier` 를 제공한다.

```python
from sklearn.ensemble import RandomForestClassifier

# (1) Random Forest
rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)
rnd_clf.fit(X_train, y_train)
y_pred_rf = rnd_clf.predict(X_test)

# (2) BaggingClassifier
bag_clf = BaggingClassifier(
    DecisionTreeClassifier(splitter="random", max_leaf_nodes=16, random_state=42),
    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=42)
bag_clf.fit(X_train, y_train)
y_pred = bag_clf.predict(X_test)

# 두 모델의 예측 비교
print(np.sum(y_pred == y_pred_rf) / len(y_pred))
# 결과 : 0.976
```

<br>

#### 4.1 엑스트라 트리(Extremely Randomized Trees)

- 랜덤 포레스트는 트리를 생성할 때, 각 노드는 랜덤하게 특성(feature)의 서브셋(자식 노드)을 만들어 분할한다. 
- 반면 `익스트림 랜덤 트리(Extremely Randomized Trees)` 또는 `엑스트라 트리(Extra-Trees)` 는 트리를 더욱 랜덤하게 생성하기 위해 노드를 분할하는 최적의 임계값을 찾는 것이 아니라, 후보 특성을 이용해 랜덤하게 분할한 다음 그 중에서 최상의 분할을 선택하는 방법
- 랜덤 포레스트처럼 각 노드의 특성마다 최적의 임계값을 찾는 것이 아니기 때문에, 엑스트라 트리가 훨씬 학습 속도가 빠르다.

```python
from sklearn.ensemble import ExtraTreesClassifier

extra_clf = ExtraTreesClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)
extra_clf.fit(X_train, y_train)
y_pred_ext = extra_clf.predict(X_test)

# 두 모델의 예측 비교
print(np.sum(y_pred_rf == y_pred_ext) / len(y_pred_rf))
# 결과 : 0.968

# y_pred_rf 는 4의 예시에서 도출했다.
```

<br>

#### 4.2 특성 중요도(Feature Importance)

- 랜덤 포레스트의 장점은 특성의 상대적인 중요도를 측정하기 쉽다는 것이다. Scikit-Learn에서는 어떠한 특성을 사용한 노드가 **불순도(impurity)**를 얼마나 감소시키는지를 계산하여 각 특성마다 상대적 중요도를 측정한다.
- Scikit-Learn의 `RandomForestClassifier` 에서 `feature_importances` 변수를 통해 해당 특성의 중요도를 확인할 수 있음

- 예시 - `RandomForestClassifier` 를 이용해 학습시키고 각 특성의 중요도를 출력하는 예제

```python
from sklearn.datasets import load_iris

iris = load_iris()
rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)
rnd_clf.fit(iris["data"], iris["target"])
for name, score in zip(iris["feature_names"], rnd_clf.feature_importances_):
    print(name, score)
```

```markdown
### 결과
sepal length (cm) 0.11249225099876374
sepal width (cm) 0.023119288282510326
petal length (cm) 0.44103046436395765
petal width (cm) 0.4233579963547681


출처: https://excelsior-cjh.tistory.com/166 [EXCELSIOR]
```

<br>

### 5. 부스팅(Boosting)

- 부스팅은 성능이 약한 학습기(weak learner)를 여러 개 연결하여 강한 학습기(strong learner)를 만드는 앙상블 방법(약한 학습기에 가중치 부여하기)
- 부스팅 방법의 아이디어는 앞에서 학습된 모델을 보완해나가면서 더 나은 모델로 학습시키는 것
- 부스팅 방법 : `AdaBoost(Adaptive Boosting)` ,`그래디언트 부스팅(Gradient Boosting)` 

<br>

#### 5.1 아다부스트(AdaBoost)

- 아다부스트는 과소적합됐던 학습 데이터 샘플의 가중치를 높이면서 새로 학습된 모델이 학습하기 어려운 데이터에 더 잘 적합되도록 하는 방식
  - Step 01. 먼저 전체 학습 데이터셋을 이용해 모델을 만든 후, 잘못 예측(분류)된 샘플의 가중치를 상대적으로 높여줌
  - Step 02. 두번째 모델을 학습 시킬 때 이렇게 업데이트된 가중치를 반영하여 모델을 학습 시킴
  - Step 03. 위와 같은 과정 반복

![](https://t1.daumcdn.net/cfile/tistory/996E3A355B836F8019?download)

- 예시 

```python
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_moons

X, y = make_moons(n_samples=500, noise=0.30, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

from sklearn.ensemble import AdaBoostClassifier

# AdaBoostClassifier (learning_rate, )
ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=1), n_estimators=200,
    algorithm="SAMME.R", learning_rate=0.5, random_state=42)
ada_clf.fit(X_train, y_train)

plot_decision_boundary(ada_clf, X, y)
```

<br>

#### 5.2 그래디언트 부스팅

- 학습된 모델의 오차를 보완하는 방향
- 학습 전 단계 모델에서의 **잔여 오차(residual error)** 에 대해 새로운 모델을 학습시키는 방법
  - cf. Adaboost 는 학습단계마다 데이터 샘플의 가중치를 업데이트 해줌

![](https://t1.daumcdn.net/cfile/tistory/99AE553A5B8370021E?download)

- 예제: 데이터에 그래디언트 부스팅 적용하기
  - 그래디언트 부스팅은 의사결정나무 알고리즘을 이용해 그래디언트 부스팅을 사용하며, 이를 그래디언트 트리 부스팅 또는 그래디언트 트리 부스팅 또는 그래디언트 부스티드 회귀 트리(GRBT) 라고 한다.

  ```python
  # DecisionTreeRegressor 그래디언트 부스팅 단계별로 확인
  np.random.seed(42)
  X = np.random.rand(100, 1) - 0.5
  y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)
  ```

  ```
  from sklearn.tree import DecisionTreeRegressor
  
  tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)
  tree_reg1.fit(X, y)
  ```

  - 처음 학습시킨 모델 `tree_reg1` 에 대한 잔여오차(residual error)를 구한 뒤 이것을 가지고 두번째 `DecisionTreeRegressor` 모델을 학습시킨다.

  ```python
  y2 = y - tree_reg1.predict(X)  # 첫번째 모델에 대한 residual errors
  
  tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)
  tree_reg2.fit(X, y2)
  ```

  - 위와 같은 방법으로 두번째 모델에 대한 잔여 오차를 구해서 세번째 모델 학습

  ```python
  y3 = y2 - tree_reg2.predict(X)  # residual error
  
  tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)
  tree_reg3.fit(X, y3)
  ```

  - 이렇게 학습시킨 그래디언트 부스팅 모델에 새로운 데이터 `X_new = 0.8` 에 대해 예측값 구하기

  ```python
  X_new = np.array([[0.8]])  # new data
  
  y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))
  print('y_pred :', y_pred)
  
  # 결과 y_pred : [0.75026781]
  ```

  - `GradientBoostingRegressor` 에서 `learning_rate` 인자는 학습 단계에서 각 모델(트리)의 반영 비율임

  ![image-20200123113857372](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200123113857372.png)

  - 이렇게 `learning_rate` 를 이용하는 방법은 **축소** 라고 하는 규제 방법

- 예시 - `n_estimators=3` 과 `n_estimators=100` 의 비교

  ```python
  from sklearn.ensemble import GradientBoostingRegressor
  
  # n_estimators = 3
  gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1, random_state=42)
  gbrt.fit(X, y)
  
  print('y_pred :', gbrt.predict(X_new))
  # 결과 : y_pred : [0.75026781]
  ```

  ```python
  # n_estimators = 200
  gbrt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=1, random_state=42)
  gbrt_slow.fit(X, y)
  ```

  ```python
  plt.figure(figsize=(11,4))
  
  plt.subplot(121)
  plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label="앙상블의 예측")
  plt.title("learning_rate={}, n_estimators={}".format(gbrt.learning_rate, gbrt.n_estimators), fontsize=14)
  
  plt.subplot(122)
  plot_predictions([gbrt_slow], X, y, axes=[-0.5, 0.5, -0.1, 0.8])
  plt.title("learning_rate={}, n_estimators={}".format(gbrt_slow.learning_rate, gbrt_slow.n_estimators), fontsize=14)
  
  plt.show()
  ```

  ![image-20200123130713419](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20200123130713419.png)

<br>

### 6. 스태킹(Stacking)

- **스태킹**(Stacking, stacked generalization의 줄임)은 앙상블 학습에서 각 모델의 예측값을 가지고 새로운 **메타 모델**을 학습시켜 최종 예측 모델을 만드는 방법

![](https://t1.daumcdn.net/cfile/tistory/998CF9435B83707B2E?download)

- 스태킹 과정
  - Step 01. 학습 데이터셋에서 샘플링을 통해 서브셋(Subset -1)을 만들고, 이 서브셋을 이용해 각 모델을 학습시킨다.
  - Step 02. 서브셋2(Subset -2) 학습 시킨 모델을 이용해 각 모델의 예측값을 출력하고, 예측값들을 합친다.
  - Step 03. 합쳐진 예측값들을 입력 특성(input feature)로 사용하는 새로운 모델(meta learner, blender)을 학습시킨다.

<br>[참고](https://github.com/ExcelsiorCJH/Hands-On-ML/blob/master/Chap07-Ensemble_Learning_and_Random_Forests/Chap07-Ensemble_Learning_and_Random_Forests.ipynb)

